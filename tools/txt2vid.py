# text-to-video.py
import torch
from diffusers import DiffusionPipeline
from moviepy.editor import ImageSequenceClip
import tkinter as tk
from tkinter import messagebox, ttk
import numpy as np

# Function to prompt user for choice
def prompt_user_choice():
    root = tk.Tk()
    root.geometry("300x600")
    root.title("Generate Image or Video")

    panel_width = 290  # 300px window width minus 10px padding
    panel_height = 500
    panel = ttk.Frame(root, width=panel_width, height=panel_height, relief='sunken')
    panel.pack(padx=5, pady=5)

    def on_image():
        root.choice = 'image'
        root.destroy()

    def on_video():
        root.choice = 'video'
        root.destroy()

    image_button = ttk.Button(panel, text="Generate Image", command=on_image)
    image_button.pack(pady=10)

    video_button = ttk.Button(panel, text="Generate Video", command=on_video)
    video_button.pack(pady=10)

    root.choice = None
    root.mainloop()
    return root.choice

# Load the pre-trained model
model_id = "CompVis/stable-diffusion-v1-4"
pipe = DiffusionPipeline.from_pretrained(model_id)
pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# Define the text prompt
prompt = "A lady walking up to the house at 8964 S Newberry Ln., Tempe, AZ, and talking about debt collection."

# Prompt user for choice
choice = prompt_user_choice()

if choice == 'image':
    # Generate the image
    image = pipe(prompt).images[0]
    
    # Save the image
    output_image_path = "output_image.png"
    image.save(output_image_path)
    
    print('output_image_path:', output_image_path)
elif choice == 'video':
    # Generate a sequence of images
    num_frames = 10
    images = [np.array(pipe(prompt).images[0]) for _ in range(num_frames)]
    
    # Save the images as a video
    output_video_path = "output_video.mp4"
    clip = ImageSequenceClip([img for img in images], fps=1)
    clip.write_videofile(output_video_path, codec="libx264")
    print('output_video_path:', output_video_path)

# Generated by Copilot